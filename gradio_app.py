#!/usr/bin/env python3
"""
Gradio Web ç•Œé¢
ä¸ºçˆ¬è™«é¡¹ç›®æä¾›å‹å¥½çš„ Web æ“ä½œç•Œé¢

åŠŸèƒ½åŒ…æ‹¬ï¼š
- çˆ¬å–æ§åˆ¶ä¸ç›‘æ§
- æ•°æ®ç»Ÿè®¡ä¸å¯è§†åŒ–
- é“¾æ¥ç®¡ç†ä¸æŸ¥è¯¢
- æ–‡ä»¶æµè§ˆä¸ä¸‹è½½

ä½œè€…ï¼šAIåŠ©æ‰‹
åˆ›å»ºæ—¶é—´ï¼š2024
"""

import gradio as gr
import asyncio
import threading
import os
import time
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import queue
import json
import sys
from matplotlib import rcParams

# Matplotlib ä¸­æ–‡æ˜¾ç¤ºä¸è´Ÿå·å¤„ç†è®¾ç½®ï¼Œé¿å…å›¾è¡¨ä¸­æ–‡å­—ä¹±ç ä¸è´Ÿå·å˜æˆæ–¹å—
# ä¼˜å…ˆä½¿ç”¨å¸¸è§çš„ä¸­æ–‡å­—ä½“ï¼ŒæŒ‰é¡ºåºå›é€€ï¼›æœ€åå›é€€åˆ° DejaVu Sansï¼ˆè¦†ç›–ä¸å…¨ä½†é€šç”¨ï¼‰
rcParams['font.sans-serif'] = [
    'Noto Sans CJK SC',      # Linux å¸¸è§
    'SimHei',                # é»‘ä½“ï¼ˆéƒ¨åˆ†ç³»ç»Ÿå¯ç”¨ï¼‰
    'WenQuanYi Zen Hei',     # æ–‡æ³‰é©¿æ­£é»‘ï¼ˆLinux å¸¸è§ï¼‰
    'Microsoft YaHei',       # å¾®è½¯é›…é»‘ï¼ˆWindowsï¼‰
    'Arial Unicode MS',      # è·¨å¹³å°å¤§å­—ç¬¦é›†
    'DejaVu Sans'            # å…œåº•å­—ä½“
]
rcParams['axes.unicode_minus'] = False

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from link_database import LinkDatabase
from ai_haodaifu import (
    _sanitize_filename, 
    url_to_file_paths, 
    extract_page_title_from_html,
    extract_page_title_from_markdown
)
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse
from docker_utils import is_docker_environment


def get_domain_without_www(domain: str) -> str:
    """è·å–åŸŸåçš„å˜ä½“ï¼ˆä¸å¸¦wwwï¼‰
    ä¾‹å¦‚ï¼š
    - è¾“å…¥: 'www.taobao.com' -> è¿”å›: 'taobao.com'
    - è¾“å…¥: 'taobao.com' -> è¿”å›: 'taobao.com'
    """
    if not domain:
        return ""
    
    domain_without_www = domain  # åŸå§‹åŸŸå
    
    if domain.startswith('www.'):
        # å¦‚æœåŸŸåä»¥wwwå¼€å¤´ï¼Œæ·»åŠ ä¸å¸¦wwwçš„ç‰ˆæœ¬
        without_www = domain[4:]  # ç§»é™¤'www.'
        domain_without_www = without_www
    
    return domain_without_www

class CrawlerManager:
    """çˆ¬è™«ç®¡ç†å™¨ï¼Œå¤„ç†å¼‚æ­¥çˆ¬è™«æ“ä½œ"""
    
    def __init__(self):
        self.db = LinkDatabase()
        self.is_running = False
        self.current_task = None
        self.log_queue = queue.Queue()
        self.progress_callback = None
        self.log_history = []  # å­˜å‚¨å†å²æ—¥å¿—
        self.max_log_history = 1000  # æœ€å¤§ä¿ç•™æ—¥å¿—æ¡æ•°
        self.log_file_path = os.path.join(os.path.dirname(__file__), "crawler_logs.txt")  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
        # å¯åŠ¨æ—¶åŠ è½½å†å²æ—¥å¿—
        self.load_logs_from_file()
        # æ¸…ç†è¿‡æ—§çš„æ—¥å¿—æ–‡ä»¶
        self._cleanup_old_logs()
        
    def log_message(self, message: str):
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆç”¨äºå®æ—¶æ˜¾ç¤ºï¼‰
        self.log_queue.put(log_entry)
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.log_history.append(log_entry)
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(self.log_history) > self.max_log_history:
            self.log_history = self.log_history[-self.max_log_history:]
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        self._write_log_to_file(log_entry)
        
        print(log_entry)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    
    def _write_log_to_file(self, log_entry: str):
        """å°†æ—¥å¿—å†™å…¥æ–‡ä»¶"""
        try:
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                f.write(log_entry + '\n')
        except Exception as e:
            print(f"å†™å…¥æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
    
    def _cleanup_old_logs(self):
        """æ¸…ç†è¿‡æ—§çš„æ—¥å¿—æ–‡ä»¶å†…å®¹"""
        try:
            if os.path.exists(self.log_file_path):
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(self.log_file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # å¦‚æœæ–‡ä»¶è¿‡å¤§ï¼ˆè¶…è¿‡5000è¡Œï¼‰ï¼Œåªä¿ç•™æœ€è¿‘çš„ä¸€åŠ
                if len(lines) > 5000:
                    lines = lines[-2500:]
                    
                    # é‡å†™æ–‡ä»¶
                    with open(self.log_file_path, 'w', encoding='utf-8') as f:
                        f.writelines(lines)
                        
        except Exception as e:
            print(f"æ¸…ç†æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        
    def set_progress_callback(self, callback):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
        
    def update_progress(self, current: int, total: int, message: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if self.progress_callback:
            self.progress_callback(current, total, message)
            
    async def crawl_single_url(self, url: str, crawler: AsyncWebCrawler, link_filters: str = "", skip_processed_check: bool = False) -> Tuple[bool, List[str]]:
        """çˆ¬å–å•ä¸ª URLï¼Œè¿”å› (æ˜¯å¦æˆåŠŸ, æ–°å‘ç°çš„é“¾æ¥åˆ—è¡¨)
        å‚æ•° skip_processed_check=True æ—¶è·³è¿‡â€œæ˜¯å¦å·²å¤„ç†è¿‡â€çš„æ£€æŸ¥ï¼ˆç”¨äºèµ·å§‹ URLï¼‰ã€‚
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆèµ·å§‹URLå¯è·³è¿‡ï¼‰
            if not skip_processed_check and self.db.is_link_processed(url):
                self.log_message(f"ğŸ” é“¾æ¥å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {url}")
                return True, []
            
            # è§£æ URL å¹¶å‡†å¤‡ä¿å­˜è·¯å¾„
            output_root = os.path.join(os.path.dirname(__file__), "output")
            tmp_md_path, tmp_html_path = url_to_file_paths(output_root, url)
            os.makedirs(os.path.dirname(tmp_md_path), exist_ok=True)
            
            # çˆ¬å–é¡µé¢
            page = await crawler.arun(url=url, config=CrawlerRunConfig())
            
            # è·å–å†…å®¹
            page_markdown = getattr(page, "markdown", None)
            page_html = getattr(page, "html", None) if hasattr(page, "html") else None
            
            # æå–é¡µé¢æ ‡é¢˜
            title_from_html = extract_page_title_from_html(page_html or "")
            title_from_md = extract_page_title_from_markdown(page_markdown or "")
            filename_stem = title_from_html or title_from_md
            
            # ç”Ÿæˆæœ€ç»ˆä¿å­˜è·¯å¾„
            md_path, html_path = url_to_file_paths(output_root, url, filename_stem_override=filename_stem)
            
            # ä¿å­˜å†…å®¹
            if page_markdown:
                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(page_markdown)
                self.log_message(f"ğŸ” å·²ä¿å­˜ Markdown â†’ {os.path.basename(md_path)}")
                self.db.update_link_success(url, filename_stem, md_path, html_path, "markdown")
            elif page_html:
                with open(html_path, "w", encoding="utf-8") as f:
                    f.write(page_html)
                self.log_message(f"ğŸ” å·²ä¿å­˜ HTML â†’ {os.path.basename(html_path)}")
                self.db.update_link_success(url, filename_stem, md_path, html_path, "html")
            else:
                # ä¿å­˜å ä½å†…å®¹
                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(f"æ— æ³•è·å–å¯ä¿å­˜çš„å†…å®¹ï¼š{url}\n")
                self.log_message(f"ğŸ” å†…å®¹ä¸ºç©ºï¼Œå ä½å†™å…¥ â†’ {os.path.basename(md_path)}")
                self.db.update_link_success(url, filename_stem, md_path, html_path, "placeholder")
            
            # ä»å½“å‰é¡µé¢æå–æ–°çš„é“¾æ¥
            new_links = []
            if page_markdown or page_html:
                self.log_message(f"ğŸ” æ­£åœ¨ä»é¡µé¢æå–æ–°é“¾æ¥...")
                extracted_links = self.extract_links_from_content(page_markdown or "", page_html or "")
                
                if extracted_links:
                    # åº”ç”¨é“¾æ¥è¿‡æ»¤ï¼ˆå¦‚æœè®¾ç½®äº†è¿‡æ»¤æ¡ä»¶ï¼‰
                    if link_filters and link_filters.strip():
                        original_count = len(extracted_links)
                        extracted_links = filter_links(extracted_links, link_filters)
                        filtered_count = len(extracted_links)
                        if original_count != filtered_count:
                            self.log_message(f"ğŸ” è¿‡æ»¤åå‰©ä½™ {filtered_count} ä¸ªæ–°é“¾æ¥ï¼ˆè¿‡æ»¤æ‰ {original_count - filtered_count} ä¸ªï¼‰")
                    
                    # ç­›é€‰å‡ºçœŸæ­£çš„æ–°é“¾æ¥ï¼ˆæœªåœ¨æ•°æ®åº“ä¸­å­˜åœ¨ï¼‰
                    for new_url in extracted_links:
                        if not self.db.is_link_exists(new_url):
                            new_links.append(new_url)
                    
                    if new_links:
                        self.log_message(f"ğŸ” å¯¹ç…§æ•°æ®åº“ï¼Œå‘ç° {filtered_count} -> {len(new_links)} ä¸ªé“¾æ¥")
                    else:
                        self.log_message(f"ğŸ” å¯¹ç…§æ•°æ®åº“ï¼Œæœªå‘ç°æ–°çš„é“¾æ¥")
                else:
                    self.log_message(f"ğŸ” é¡µé¢ä¸­æœªå‘ç°é“¾æ¥")
            
            return True, new_links
            
        except Exception as e:
            error_msg = str(e)
            self.log_message(f"âŒ çˆ¬å–å¤±è´¥ï¼š{url}")
            self.log_message(f"   åŸå› ï¼š{error_msg}")
            self.db.update_link_failed(url, error_msg)
            return False, []
    
    def extract_links_from_content(self, markdown_text: str, html_text: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–é“¾æ¥"""
        all_links = []
        
        # ä» Markdown æå–é“¾æ¥
        if markdown_text:
            # åŒ¹é… [text](url) å½¢å¼çš„é“¾æ¥
            pattern_md_link = re.compile(r"\[[^\]]*\]\((https?://[^\s)]+)\)")
            for m in pattern_md_link.finditer(markdown_text):
                url = m.group(1).split('#', 1)[0]  # å»æ‰ç‰‡æ®µæ ‡è¯†ç¬¦
                all_links.append(url)
            
            # åŒ¹é…è£¸éœ² URL
            pattern_bare_url = re.compile(r"(https?://[^\s)]+)")
            for m in pattern_bare_url.finditer(markdown_text):
                url = m.group(1).split('#', 1)[0]
                all_links.append(url)
        
        # ä» HTML æå–é“¾æ¥
        if html_text:
            soup = BeautifulSoup(html_text, "html.parser")
            for a in soup.find_all("a"):
                href = a.get("href")
                if href and isinstance(href, str):
                    if href.startswith("http://") or href.startswith("https://"):
                        url = href.split('#', 1)[0]
                        all_links.append(url)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        return list(dict.fromkeys(all_links))
    
    async def crawl_from_url(self, start_url: str, headless: bool = True, link_filters: str = "", max_depth: int = 3):
        """ä»èµ·å§‹ URL å¼€å§‹é€’å½’çˆ¬å–"""
        if self.is_running:
            self.log_message("çˆ¬è™«å·²åœ¨è¿è¡Œä¸­ï¼Œè¯·å…ˆåœæ­¢å½“å‰ä»»åŠ¡")
            return
        
        self.is_running = True
        self.log_message(f"ğŸš€ å¼€å§‹é€’å½’çˆ¬å–ä»»åŠ¡")
        self.log_message(f"ğŸ“ èµ·å§‹URL: {start_url}")
        self.log_message(f"âš™ï¸ é…ç½®: æ— å¤´æ¨¡å¼={headless}, æœ€å¤§é€’å½’æ·±åº¦={max_depth}")
        
        try:
            # é…ç½®æµè§ˆå™¨
            self.log_message("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
            browser_cfg = BrowserConfig(headless=headless)
            
            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                # åˆå§‹åŒ–çˆ¬å–é˜Ÿåˆ—
                crawl_queue = []  # å¾…çˆ¬å–é“¾æ¥é˜Ÿåˆ—
                processed_count = 0  # å·²å¤„ç†é“¾æ¥è®¡æ•°
                success_count = 0    # æˆåŠŸçˆ¬å–è®¡æ•°
                
                # ä»æ•°æ®åº“ä¸­è·å–å¾…å¤„ç†çš„é“¾æ¥
                pending_links = self.db.get_links_by_status('pending')
                for link in pending_links:
                    crawl_queue.append((link['url'], 0))
                
                # å°†èµ·å§‹URLæ·»åŠ åˆ°é˜Ÿåˆ—å¹¶è®°å½•åˆ°æ•°æ®åº“
                crawl_queue.append((start_url, 0))  # (url, depth)
                
                # è®°å½•èµ·å§‹URLåˆ°æ•°æ®åº“
                try:
                    self.db.record_link_discovered(start_url)
                except Exception as e:
                    self.log_message(f"è®°å½•èµ·å§‹URLåˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                self.log_message(f"ğŸ“‹ å·²å°†èµ·å§‹URLæ·»åŠ åˆ°çˆ¬å–é˜Ÿåˆ—")
                
                # å¼€å§‹é€’å½’çˆ¬å–
                while crawl_queue and self.is_running:
                    self.log_message("---------------start-----------------")
                    # ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸€ä¸ªé“¾æ¥
                    current_url, current_depth = crawl_queue.pop(0)
                    processed_count += 1
                    
                    self.log_message(f"ğŸ“ é˜Ÿåˆ—[{processed_count}] æ­£åœ¨å¤„ç† (æ·±åº¦{current_depth}): {current_url}")
                    self.update_progress(processed_count, processed_count + len(crawl_queue), f"æ­£åœ¨å¤„ç†: {current_url}")
                    
                    # çˆ¬å–å½“å‰é“¾æ¥
                    # æ·»åŠ å½“å‰åŸŸåçš„å˜ä½“ï¼ˆå¸¦wwwå’Œä¸å¸¦wwwï¼‰
                    current_domain = urlparse(current_url).netloc
                    domain_variants = get_domain_without_www(current_domain)
                    if domain_variants not in link_filters:
                        link_filters = link_filters + "," + domain_variants
                    
                    self.log_message(f"ğŸ” é“¾æ¥è¿‡æ»¤æ¡ä»¶: {link_filters}")
                    
                    success, new_links = await self.crawl_single_url(
                        current_url,
                        crawler,
                        link_filters,
                        skip_processed_check=(current_depth == 0)
                    )

                    if success:
                        success_count += 1
                        self.log_message(f"âœ… é˜Ÿåˆ—[{processed_count}] å¤„ç†æˆåŠŸ")
                        
                        # å¦‚æœè¿˜æ²¡è¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œå°†æ–°å‘ç°çš„é“¾æ¥æ·»åŠ åˆ°é˜Ÿåˆ—
                        if current_depth < max_depth and new_links:
                            # å°†æ–°é“¾æ¥æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆæ·±åº¦+1ï¼‰
                            added_count = 0
                            for new_url in new_links:
                                if not self.is_url_in_queue(crawl_queue, new_url):
                                    try:
                                        crawl_queue.append((new_url, current_depth + 1))
                                        self.db.record_link_discovered(new_url)
                                        self.log_message(f"ğŸ†• é˜Ÿåˆ—[{processed_count}] å‘ç°æ–°é“¾æ¥: {new_url}")
                                    except Exception as e:
                                        self.log_message(f"è®°å½•æ–°é“¾æ¥åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                                    added_count += 1
                            
                            if added_count > 0:
                                self.log_message(f"ğŸ†• {len(new_links)} ä¸ªæ–°é“¾æ¥ï¼Œå·²æ·»åŠ  {added_count} å…¥åº“ï¼Œå¹¶æ·»åŠ åˆ°åˆ°é˜Ÿåˆ—ä¸­ï¼ˆæ·±åº¦ {current_depth + 1}ï¼‰")
                        elif current_depth >= max_depth:
                            self.log_message(f"ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ {max_depth}ï¼Œä¸å†ç»§ç»­æ·±å…¥")
                    else:
                        self.log_message(f"âŒ é˜Ÿåˆ—[{processed_count}] å¤„ç†å¤±è´¥")
                    
                    # æ·»åŠ å°å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    await asyncio.sleep(0.5)
                    
                    # æ˜¾ç¤ºå½“å‰é˜Ÿåˆ—çŠ¶æ€
                    if len(crawl_queue) > 0:
                        self.log_message(f"ğŸ“‹ é˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(crawl_queue)} ä¸ªé“¾æ¥å¾…å¤„ç†")
                    self.log_message("---------------- end ----------------")
                
                # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
                if not self.is_running:
                    self.log_message("â¹ï¸ çˆ¬å–ä»»åŠ¡è¢«ç”¨æˆ·åœæ­¢")
                else:
                    self.log_message("ğŸ‰ é€’å½’çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
                
                self.log_message(f"ğŸ“Š é˜Ÿåˆ—ç»Ÿè®¡: æˆåŠŸ {success_count}/{processed_count}")
                stats = self.db.get_crawl_statistics()
                self.log_message(f"ğŸ—„ï¸ æ•°æ®åº“ç»Ÿè®¡ - æ€»è®¡: {stats['total']}, æˆåŠŸ: {stats['success']}, å¤±è´¥: {stats['failed']}")
                
        except Exception as e:
            self.log_message(f"ğŸ’¥ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        finally:
            self.is_running = False
            self.log_message("ğŸ çˆ¬å–ä»»åŠ¡ç»“æŸ")
    
    def is_url_in_queue(self, queue: List[Tuple[str, int]], url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²åœ¨é˜Ÿåˆ—ä¸­"""
        return any(queue_url == url for queue_url, _ in queue)
    
    def stop_crawling(self):
        """åœæ­¢çˆ¬å–"""
        if self.is_running:
            self.is_running = False
            self.log_message("æ­£åœ¨åœæ­¢çˆ¬å–ä»»åŠ¡...")
            return "å·²å‘é€åœæ­¢ä¿¡å·"
        else:
            return "å½“å‰æ²¡æœ‰è¿è¡Œä¸­çš„çˆ¬å–ä»»åŠ¡"
    
    def get_logs(self) -> str:
        """è·å–æ—¥å¿—å†…å®¹"""
        # è·å–é˜Ÿåˆ—ä¸­çš„æ–°æ—¥å¿—
        new_logs = []
        while not self.log_queue.empty():
            try:
                new_logs.append(self.log_queue.get_nowait())
            except queue.Empty:
                break
        
        # å¦‚æœæ²¡æœ‰æ–°æ—¥å¿—ï¼Œè¿”å›å†å²æ—¥å¿—
        if not new_logs and not self.log_history:
            return "[ç³»ç»Ÿ] çˆ¬è™«ç®¡ç†å™¨å·²å°±ç»ª"
        
        # åˆå¹¶å†å²æ—¥å¿—å’Œæ–°æ—¥å¿—
        all_logs = self.log_history + new_logs
        
        # è¿”å›æœ€è¿‘500æ¡æ—¥å¿—
        return "\n".join(all_logs[-500:])
    
    def clear_logs(self):
        """æ¸…ç©ºæ—¥å¿—å†å²"""
        self.log_history.clear()
        # æ¸…ç©ºé˜Ÿåˆ—
        while not self.log_queue.empty():
            try:
                self.log_queue.get_nowait()
            except queue.Empty:
                break
        self.log_message("æ—¥å¿—å·²æ¸…ç©º")
    
    def load_logs_from_file(self):
        """ä»æ—¥å¿—æ–‡ä»¶åŠ è½½å†å²æ—¥å¿—"""
        try:
            if os.path.exists(self.log_file_path):
                with open(self.log_file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # åŠ è½½æœ€è¿‘çš„æ—¥å¿—åˆ°å†å²è®°å½•
                recent_lines = lines[-self.max_log_history:] if len(lines) > self.max_log_history else lines
                self.log_history = [line.strip() for line in recent_lines if line.strip()]
                
        except Exception as e:
            print(f"åŠ è½½æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")


# å…¨å±€çˆ¬è™«ç®¡ç†å™¨å®ä¾‹
crawler_manager = CrawlerManager()


def validate_url(url: str) -> str:
    """éªŒè¯ URL æ ¼å¼"""
    if not url or not url.strip():
        raise gr.Error("è¯·è¾“å…¥ URL")
    
    url = url.strip()
    if not url.startswith(('http://', 'https://')):
        raise gr.Error("è¯·è¾“å…¥æœ‰æ•ˆçš„ URLï¼ˆä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰")
    
    return url


def validate_link_filters(filter_text: str) -> str:
    """éªŒè¯é“¾æ¥è¿‡æ»¤æ¡ä»¶"""
    if not filter_text or not filter_text.strip():
        return ""  # ç©ºè¿‡æ»¤æ¡ä»¶æœ‰æ•ˆ
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    keywords = [keyword.strip() for keyword in filter_text.split(',')]
    if any(len(keyword) == 0 for keyword in keywords):
        raise gr.Error("è¿‡æ»¤å…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼Œè¯·æ£€æŸ¥é€—å·åˆ†éš”çš„æ ¼å¼")
    
    return filter_text.strip()


def filter_links(links: List[str], filter_text: str) -> List[str]:
    """æ ¹æ®åŒ…å«å­—ç¬¦è¿‡æ»¤é“¾æ¥åˆ—è¡¨,æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªå…³é”®è¯"""
    if not filter_text or not filter_text.strip():
        return links  # å¦‚æœæ²¡æœ‰è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰é“¾æ¥
    
    # è§£æè¿‡æ»¤æ¡ä»¶ï¼ˆæ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªå…³é”®è¯ï¼‰
    keywords = [keyword.strip().lower() for keyword in filter_text.split(',') if keyword.strip()]
    
    filtered_links = []
    for link in links:
        link_lower = link.lower()
        # æ£€æŸ¥é“¾æ¥æ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®è¯
        if all(keyword in link_lower for keyword in keywords):
            filtered_links.append(link)
    
    return filtered_links


def format_url_for_display(url: str, max_length: int = 80) -> str:
    """æ ¼å¼åŒ–URLç”¨äºæ˜¾ç¤ºï¼Œæ ¹æ®é•¿åº¦æ™ºèƒ½æˆªæ–­"""
    if not url:
        return ""
    
    url_length = len(url)
    
    # å¦‚æœURLé•¿åº¦å°äºç­‰äºæœ€å¤§é•¿åº¦ï¼Œç›´æ¥è¿”å›
    if url_length <= max_length:
        return url
    
    # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™åè®®ã€åŸŸåå’Œè·¯å¾„çš„é‡è¦éƒ¨åˆ†
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        
        # æ„å»ºåŸºç¡€éƒ¨åˆ†ï¼ˆåè®® + åŸŸåï¼‰
        base = f"{parsed.scheme}://{parsed.netloc}"
        path = parsed.path
        query = parsed.query
        fragment = parsed.fragment
        
        # å¦‚æœåŸºç¡€éƒ¨åˆ†å·²ç»å¾ˆé•¿ï¼Œç›´æ¥æˆªæ–­
        if len(base) > max_length - 10:
            return url[:max_length-3] + "..."
        
        # è®¡ç®—å‰©ä½™å¯ç”¨é•¿åº¦
        remaining_length = max_length - len(base) - 3  # 3 for "..."
        
        # å¦‚æœæœ‰æŸ¥è¯¢å‚æ•°æˆ–ç‰‡æ®µï¼Œä¼˜å…ˆä¿ç•™
        if query or fragment:
            # ä¿ç•™è·¯å¾„çš„ä¸€éƒ¨åˆ†å’ŒæŸ¥è¯¢å‚æ•°
            if len(path) > remaining_length // 2:
                path = path[:remaining_length//2] + "..."
            query_part = f"?{query}" if query else ""
            fragment_part = f"#{fragment}" if fragment else ""
            return base + path + query_part + fragment_part
        else:
            # åªæœ‰è·¯å¾„ï¼Œæ™ºèƒ½æˆªæ–­
            if len(path) > remaining_length:
                # å°è¯•ä¿ç•™è·¯å¾„çš„å¼€å¤´å’Œç»“å°¾
                if remaining_length > 20:
                    start_len = remaining_length // 2
                    end_len = remaining_length - start_len - 3
                    return base + path[:start_len] + "..." + path[-end_len:]
                else:
                    return base + path[:remaining_length] + "..."
            else:
                return base + path
                
    except Exception:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æˆªæ–­
        return url[:max_length-3] + "..."


def start_crawling(url: str, headless: bool, link_filters: str = "", max_depth: int = 10) -> Tuple[str, str]:
    """å¼€å§‹çˆ¬å–"""
    try:
        # éªŒè¯è¾“å…¥å‚æ•°
        validated_url = validate_url(url)
        validated_filters = validate_link_filters(link_filters)

        # æ¸…ç©ºcrawler_logs
        crawler_manager.clear_logs()
        crawler_manager.log_message("============ å¼€å§‹çˆ¬å– ============")
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥çˆ¬è™«
        def run_crawler():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(
                    crawler_manager.crawl_from_url(validated_url, headless, validated_filters, max_depth)
                )
            finally:
                loop.close()
        
        thread = threading.Thread(target=run_crawler)
        thread.daemon = True
        thread.start()
        
        return "é€’å½’çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨", "æ­£åœ¨åˆå§‹åŒ–..."
        
    except gr.Error as e:
        return f"è¾“å…¥é”™è¯¯: {str(e)}", ""
    except Exception as e:
        return f"å¯åŠ¨å¤±è´¥: {str(e)}", ""


def stop_crawling() -> str:
    """åœæ­¢çˆ¬å–"""
    return crawler_manager.stop_crawling()


def get_crawling_logs() -> str:
    """è·å–çˆ¬å–æ—¥å¿—"""
    return crawler_manager.get_logs()


def clear_crawling_logs() -> str:
    """æ¸…ç©ºçˆ¬å–æ—¥å¿—"""
    crawler_manager.clear_logs()
    return "æ—¥å¿—å·²æ¸…ç©º"


def get_buttons_state():
    """æ ¹æ®è¿è¡ŒçŠ¶æ€è¿”å›æŒ‰é’®å¯äº¤äº’æ€§è®¾ç½®"""
    running = crawler_manager.is_running
    # running=True: ç¦ç”¨å¼€å§‹ï¼Œå¯ç”¨åœæ­¢ï¼›åä¹‹äº¦ç„¶
    start_update = gr.update(interactive=not running)
    stop_update = gr.update(interactive=running)
    return start_update, stop_update


def start_crawling_and_update_buttons(url: str, headless: bool, link_filters: str = "", max_depth: int = 10):
    """å¼€å§‹çˆ¬å–å¹¶æ›´æ–°å¼€å§‹/åœæ­¢æŒ‰é’®çš„å¯äº¤äº’çŠ¶æ€"""
    status_msg, logs_msg = start_crawling(url, headless, link_filters, max_depth)
    start_update, stop_update = get_buttons_state()
    return status_msg, logs_msg, start_update, stop_update


def stop_crawling_and_update_buttons():
    """åœæ­¢çˆ¬å–å¹¶æ›´æ–°å¼€å§‹/åœæ­¢æŒ‰é’®çš„å¯äº¤äº’çŠ¶æ€"""
    status_msg = stop_crawling()
    start_update, stop_update = get_buttons_state()
    return status_msg, start_update, stop_update


def get_statistics() -> Tuple[int, int, int, int, float]:
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    stats = crawler_manager.db.get_crawl_statistics()
    success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
    return (
        stats['total'],
        stats['success'],
        stats['failed'],
        stats['pending'],
        success_rate
    )


def get_links_by_status(status: str = None) -> pd.DataFrame:
    """æ ¹æ®çŠ¶æ€è·å–é“¾æ¥åˆ—è¡¨ï¼Œå¦‚æœstatusä¸ºNoneã€ç©ºå­—ç¬¦ä¸²æˆ–'None'åˆ™è·å–æ‰€æœ‰é“¾æ¥"""
    # å¦‚æœstatusæ˜¯å­—ç¬¦ä¸²'None'ï¼Œè½¬æ¢ä¸ºNone
    if status == 'None':
        status = None
    links = crawler_manager.db.get_links_by_status(status)
    if not links:
        return pd.DataFrame(columns=['URL', 'æ ‡é¢˜', 'çŠ¶æ€', 'å‘ç°æ—¶é—´', 'æŠ“å–æ—¶é—´', 'é”™è¯¯ä¿¡æ¯'])
    
    # è½¬æ¢ä¸º DataFrame
    data = []
    for link in links:
        # ä½¿ç”¨æ™ºèƒ½URLæ ¼å¼åŒ–å‡½æ•°
        display_url = format_url_for_display(link['url'], max_length=100)
            
        data.append({
            'URL': display_url,
            'æ ‡é¢˜': link.get('title', 'æ— '),
            'çŠ¶æ€': link['status'],
            'å‘ç°æ—¶é—´': link['discovered_at'],
            'æŠ“å–æ—¶é—´': link.get('crawled_at', 'æœªæŠ“å–'),
            'é”™è¯¯ä¿¡æ¯': link.get('error_message', 'æ— ')
        })
    
    return pd.DataFrame(data)


def search_links(query: str) -> pd.DataFrame:
    """æœç´¢é“¾æ¥"""
    if not query or not query.strip():
        return get_links_by_status('success')  # é»˜è®¤æ˜¾ç¤ºæˆåŠŸçš„é“¾æ¥
    
    # ç®€å•çš„ URL åŒ…å«æœç´¢
    all_links = crawler_manager.db.get_recent_links(1000)  # è·å–æœ€è¿‘1000ä¸ªé“¾æ¥
    filtered_links = [link for link in all_links if query.lower() in link['url'].lower()]
    
    if not filtered_links:
        return pd.DataFrame(columns=['URL', 'æ ‡é¢˜', 'çŠ¶æ€', 'å‘ç°æ—¶é—´', 'æŠ“å–æ—¶é—´', 'é”™è¯¯ä¿¡æ¯'])
    
    # è½¬æ¢ä¸º DataFrame
    data = []
    for link in filtered_links:
        # ä½¿ç”¨æ™ºèƒ½URLæ ¼å¼åŒ–å‡½æ•°
        display_url = format_url_for_display(link['url'], max_length=100)
            
        data.append({
            'URL': display_url,
            'æ ‡é¢˜': link.get('title', 'æ— '),
            'çŠ¶æ€': link['status'],
            'å‘ç°æ—¶é—´': link['discovered_at'],
            'æŠ“å–æ—¶é—´': link.get('crawled_at', 'æœªæŠ“å–'),
            'é”™è¯¯ä¿¡æ¯': link.get('error_message', 'æ— ')
        })
    
    return pd.DataFrame(data)


def create_statistics_plot() -> plt.Figure:
    """åˆ›å»ºç»Ÿè®¡å›¾è¡¨"""
    stats = crawler_manager.db.get_crawl_statistics()
    
    # åˆ›å»ºé¥¼å›¾
    fig, ax = plt.subplots(figsize=(8, 6))
    
    if stats['total'] > 0:
        labels = ['æˆåŠŸ', 'å¤±è´¥', 'å¾…å¤„ç†']
        sizes = [stats['success'], stats['failed'], stats['pending']]
        colors = ['#2ecc71', '#e74c3c', '#f39c12']
        
        # è¿‡æ»¤æ‰0å€¼
        filtered_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]
        if filtered_data:
            labels, sizes, colors = zip(*filtered_data)
            
            wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            
            # ç¾åŒ–æ–‡æœ¬
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
    else:
        ax.text(0.5, 0.5, 'æš‚æ— æ•°æ®', ha='center', va='center', fontsize=16)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
    
    ax.set_title('æŠ“å–çŠ¶æ€åˆ†å¸ƒ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    return fig


def get_output_files() -> List[str]:
    """è·å–è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
    output_dir = os.path.join(os.path.dirname(__file__), "output")
    if not os.path.exists(output_dir):
        return []
    
    files = []
    for root, dirs, filenames in os.walk(output_dir):
        for filename in filenames:
            if filename.endswith(('.md', '.html')):
                rel_path = os.path.relpath(os.path.join(root, filename), output_dir)
                files.append(rel_path)
    
    return sorted(files)


def preview_file(file_path: str, preview_mode: str = "Markdown æ¸²æŸ“") -> str:
    """é¢„è§ˆæ–‡ä»¶å†…å®¹ï¼ˆä¸æˆªæ–­ï¼Œå®Œæ•´æ˜¾ç¤ºï¼Œæ”¯æŒ Markdown æ¸²æŸ“å’ŒåŸå§‹æ–‡æœ¬æ¨¡å¼ï¼‰"""
    if not file_path:
        return "è¯·é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶"
    
    output_dir = os.path.join(os.path.dirname(__file__), "output")
    full_path = os.path.join(output_dir, file_path)
    
    if not os.path.exists(full_path):
        return "âŒ æ–‡ä»¶ä¸å­˜åœ¨"
    
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(full_path)
        file_size_str = format_file_size(file_size)
        
        with open(full_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯å¤´éƒ¨
        file_info = f"ğŸ“„ **æ–‡ä»¶ä¿¡æ¯**\n- è·¯å¾„: `{file_path}`\n- å¤§å°: {file_size_str}\n- ç±»å‹: {get_file_type(file_path)}\n- æ¨¡å¼: {preview_mode}\n\n---\n\n"
        
        # å¦‚æœæ˜¯åŸå§‹æ–‡æœ¬æ¨¡å¼ï¼Œç›´æ¥è¿”å›åŸå§‹å†…å®¹
        if preview_mode == "åŸå§‹æ–‡æœ¬":
            if file_path.endswith('.html'):
                return f"{file_info}```html\n{content}\n```"
            elif file_path.endswith('.md'):
                return f"{file_info}```markdown\n{content}\n```"
            else:
                return f"{file_info}```\n{content}\n```"
        
        # Markdown æ¸²æŸ“æ¨¡å¼
        if file_path.endswith('.md'):
            # Markdown æ–‡ä»¶å¤„ç†ï¼Œç§»é™¤å›¾ç‰‡å¼•ç”¨
            cleaned_content = remove_images_from_markdown(content)
            return file_info + cleaned_content
        elif file_path.endswith('.html'):
            # HTML æ–‡ä»¶è½¬æ¢ä¸º Markdown æ ¼å¼
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title = soup.find('title')
                title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
                
                # æå–ä¸»è¦å†…å®¹
                main_content = soup.find('main') or soup.find('article') or soup.find('body')
                if main_content:
                    # ç®€å•çš„ HTML åˆ° Markdown è½¬æ¢
                    markdown_content = html_to_markdown_simple(main_content)
                else:
                    markdown_content = html_to_markdown_simple(soup)
                
                # ç»„åˆæœ€ç»ˆå†…å®¹
                result = f"{file_info}# {title_text}\n\n{markdown_content}"
                return result
                
            except Exception as e:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸå§‹ HTML å†…å®¹ï¼ˆç”¨ä»£ç å—åŒ…è£…ï¼‰
                return f"{file_info}âš ï¸ HTML è½¬æ¢å¤±è´¥ï¼Œæ˜¾ç¤ºåŸå§‹å†…å®¹ï¼š\n\n```html\n{content}\n```"
        else:
            # å…¶ä»–æ–‡ä»¶ç±»å‹ï¼Œç”¨ä»£ç å—åŒ…è£…
            return f"{file_info}```\n{content}\n```"
            
    except UnicodeDecodeError:
        return f"{file_info}âŒ æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œæ— æ³•è¯»å–æ–‡æœ¬å†…å®¹"
    except Exception as e:
        return f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"


def format_file_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes / (1024 * 1024):.1f} MB"
    else:
        return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"


def get_file_type(file_path: str) -> str:
    """è·å–æ–‡ä»¶ç±»å‹æè¿°"""
    if file_path.endswith('.md'):
        return "Markdown æ–‡æ¡£"
    elif file_path.endswith('.html'):
        return "HTML ç½‘é¡µ"
    elif file_path.endswith('.txt'):
        return "çº¯æ–‡æœ¬"
    else:
        return "æœªçŸ¥ç±»å‹"


def download_file(file_path: str) -> str:
    """ä¸‹è½½æ–‡ä»¶åŠŸèƒ½ï¼ˆè¿”å›æ–‡ä»¶è·¯å¾„ä¾› Gradio ä¸‹è½½ï¼‰"""
    if not file_path:
        return None
    
    output_dir = os.path.join(os.path.dirname(__file__), "output")
    full_path = os.path.join(output_dir, file_path)
    
    if not os.path.exists(full_path):
        return None
    
    return full_path


def remove_images_from_markdown(content: str) -> str:
    """ä» Markdown å†…å®¹ä¸­ç§»é™¤å›¾ç‰‡å¼•ç”¨ï¼Œç”¨æ–‡æœ¬æ›¿ä»£"""
    import re
    
    # åŒ¹é… Markdown å›¾ç‰‡è¯­æ³• ![alt](src "title")
    img_pattern = r'!\[([^\]]*)\]\(([^)]+)(?:\s+"([^"]*)")?\)'
    
    def replace_image(match):
        alt_text = match.group(1) if match.group(1) else ""
        src = match.group(2)
        title = match.group(3) if match.group(3) else ""
        
        if alt_text:
            return f"ğŸ–¼ï¸ [å›¾ç‰‡: {alt_text}]"
        elif title:
            return f"ğŸ–¼ï¸ [å›¾ç‰‡: {title}]"
        else:
            return f"ğŸ–¼ï¸ [å›¾ç‰‡: {src}]"
    
    # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
    cleaned_content = re.sub(img_pattern, replace_image, content)
    
    # ä¹Ÿå¤„ç† HTML æ ¼å¼çš„å›¾ç‰‡æ ‡ç­¾ï¼ˆå¦‚æœ Markdown ä¸­åŒ…å«ï¼‰
    html_img_pattern = r'<img[^>]*alt=["\']([^"\']*)["\'][^>]*>'
    cleaned_content = re.sub(html_img_pattern, r'ğŸ–¼ï¸ [å›¾ç‰‡: \1]', cleaned_content)
    
    # å¤„ç†æ²¡æœ‰ alt å±æ€§çš„ HTML å›¾ç‰‡
    html_img_pattern_no_alt = r'<img[^>]*src=["\']([^"\']*)["\'][^>]*>'
    cleaned_content = re.sub(html_img_pattern_no_alt, r'ğŸ–¼ï¸ [å›¾ç‰‡: \1]', cleaned_content)
    
    return cleaned_content


def html_to_markdown_simple(element) -> str:
    """ç®€å•çš„ HTML åˆ° Markdown è½¬æ¢ï¼ˆä¸æ¸²æŸ“å›¾ç‰‡ï¼‰"""
    if not element:
        return ""
    
    result = []
    
    for child in element.children:
        if hasattr(child, 'name') and child.name:
            tag_name = child.name.lower()
            text = child.get_text().strip()
            
            # è·³è¿‡å›¾ç‰‡æ ‡ç­¾ï¼Œä¸æ¸²æŸ“å›¾ç‰‡
            if tag_name == 'img':
                # æ˜¾ç¤ºå›¾ç‰‡çš„æ›¿ä»£æ–‡æœ¬æˆ–è·¯å¾„ä¿¡æ¯
                alt_text = child.get('alt', '')
                src = child.get('src', '')
                if alt_text:
                    result.append(f"ğŸ–¼ï¸ [å›¾ç‰‡: {alt_text}]")
                elif src:
                    result.append(f"ğŸ–¼ï¸ [å›¾ç‰‡: {src}]")
                else:
                    result.append("ğŸ–¼ï¸ [å›¾ç‰‡]")
                continue
            
            # è·³è¿‡å…¶ä»–åª’ä½“å…ƒç´ 
            if tag_name in ['video', 'audio', 'iframe', 'embed', 'object']:
                result.append(f"ğŸ“º [åª’ä½“å†…å®¹: {tag_name}]")
                continue
            
            if not text:  # è·³è¿‡ç©ºæ–‡æœ¬
                continue
                
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag_name[1])
                result.append(f"{'#' * level} {text}")
            elif tag_name == 'p':
                result.append(f"{text}")
            elif tag_name == 'a':
                href = child.get('href', '')
                if href:
                    result.append(f"[{text}]({href})")
                else:
                    result.append(text)
            elif tag_name in ['strong', 'b']:
                result.append(f"**{text}**")
            elif tag_name in ['em', 'i']:
                result.append(f"*{text}*")
            elif tag_name == 'code':
                result.append(f"`{text}`")
            elif tag_name == 'pre':
                result.append(f"```\n{text}\n```")
            elif tag_name == 'ul':
                for li in child.find_all('li', recursive=False):
                    result.append(f"- {li.get_text().strip()}")
            elif tag_name == 'ol':
                for i, li in enumerate(child.find_all('li', recursive=False), 1):
                    result.append(f"{i}. {li.get_text().strip()}")
            elif tag_name == 'blockquote':
                result.append(f"> {text}")
            elif tag_name == 'hr':
                result.append("---")
            else:
                # å¯¹äºå…¶ä»–æ ‡ç­¾ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                child_markdown = html_to_markdown_simple(child)
                if child_markdown:
                    result.append(child_markdown)
        else:
            # å¤„ç†æ–‡æœ¬èŠ‚ç‚¹
            text = str(child).strip()
            if text:
                result.append(text)
    
    return '\n\n'.join(result)


def retry_failed_links() -> str:
    """é‡æ–°æŠ“å–å¤±è´¥çš„é“¾æ¥"""
    failed_links = crawler_manager.db.get_links_by_status('failed')
    if not failed_links:
        return "æ²¡æœ‰å¤±è´¥çš„é“¾æ¥éœ€è¦é‡æ–°æŠ“å–"
    
    # é‡ç½®å¤±è´¥é“¾æ¥çš„çŠ¶æ€ä¸ºå¾…å¤„ç†
    count = 0
    import sqlite3
    try:
        with sqlite3.connect(crawler_manager.db.db_path) as conn:
            cursor = conn.cursor()
            for link in failed_links:
                try:
                    cursor.execute("""
                        UPDATE crawled_links 
                        SET status = 'pending', error_message = NULL, crawled_at = NULL
                        WHERE url = ?
                    """, (link['url'],))
                    count += 1
                except Exception as e:
                    print(f"é‡ç½®é“¾æ¥çŠ¶æ€å¤±è´¥: {link['url']}, é”™è¯¯: {e}")
            conn.commit()
    except Exception as e:
        return f"é‡ç½®å¤±è´¥é“¾æ¥çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    return f"å·²é‡ç½® {count} ä¸ªå¤±è´¥é“¾æ¥çš„çŠ¶æ€ä¸ºå¾…å¤„ç†"


def clear_all_links(confirm: bool = False) -> str:
    """æ¸…ç©ºæ‰€æœ‰é“¾æ¥æ•°æ®"""
    if not confirm:
        return "âš ï¸ è¯·ç¡®è®¤æ˜¯å¦è¦æ¸…ç©ºæ‰€æœ‰é“¾æ¥æ•°æ®ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ï¼\nç‚¹å‡»ç¡®è®¤åå†æ¬¡ç‚¹å‡»æ¸…ç©ºæŒ‰é’®ã€‚"
    
    import sqlite3
    try:
        with sqlite3.connect(crawler_manager.db.db_path) as conn:
            cursor = conn.cursor()
            
            # è·å–æ¸…ç©ºå‰çš„ç»Ÿè®¡ä¿¡æ¯
            cursor.execute("SELECT COUNT(*) FROM crawled_links")
            total_count = cursor.fetchone()[0]
            
            if total_count == 0:
                return "æ•°æ®åº“ä¸­æ²¡æœ‰é“¾æ¥æ•°æ®éœ€è¦æ¸…ç©º"
            
            # æ¸…ç©ºæ‰€æœ‰é“¾æ¥æ•°æ®
            cursor.execute("DELETE FROM crawled_links")
            conn.commit()
            
            return f"âœ… å·²æˆåŠŸæ¸…ç©º {total_count} æ¡é“¾æ¥è®°å½•"
            
    except Exception as e:
        return f"âŒ æ¸…ç©ºé“¾æ¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def confirm_clear_links() -> str:
    """ç¡®è®¤æ¸…ç©ºé“¾æ¥æ•°æ®"""
    return clear_all_links(confirm=True)


def clear_all_files(confirm: bool = False) -> str:
    """æ¸…ç©ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
    if not confirm:
        return "âš ï¸ è¯·ç¡®è®¤æ˜¯å¦è¦æ¸…ç©ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ï¼\nç‚¹å‡»ç¡®è®¤åå†æ¬¡ç‚¹å‡»æ¸…ç©ºæŒ‰é’®ã€‚"
    
    output_dir = os.path.join(os.path.dirname(__file__), "output")
    if not os.path.exists(output_dir):
        return "è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º"
    
    try:
        import shutil
        
        # è·å–æ¸…ç©ºå‰çš„æ–‡ä»¶ç»Ÿè®¡
        file_count = 0
        total_size = 0
        
        for root, dirs, files in os.walk(output_dir):
            for file in files:
                if file.endswith(('.md', '.html')):
                    file_path = os.path.join(root, file)
                    file_count += 1
                    total_size += os.path.getsize(file_path)
        
        if file_count == 0:
            return "è¾“å‡ºç›®å½•ä¸­æ²¡æœ‰æ–‡ä»¶éœ€è¦æ¸…ç©º"
        
        # æ¸…ç©ºè¾“å‡ºç›®å½•
        shutil.rmtree(output_dir)
        os.makedirs(output_dir, exist_ok=True)
        
        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        if total_size < 1024:
            size_str = f"{total_size} B"
        elif total_size < 1024 * 1024:
            size_str = f"{total_size / 1024:.1f} KB"
        else:
            size_str = f"{total_size / (1024 * 1024):.1f} MB"
        
        return f"âœ… å·²æˆåŠŸæ¸…ç©º {file_count} ä¸ªæ–‡ä»¶ï¼ˆæ€»å¤§å°: {size_str}ï¼‰"
        
    except Exception as e:
        return f"âŒ æ¸…ç©ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def confirm_clear_files() -> str:
    """ç¡®è®¤æ¸…ç©ºæ–‡ä»¶æ•°æ®"""
    return clear_all_files(confirm=True)


# åˆ›å»º Gradio ç•Œé¢
def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        theme=gr.themes.Soft(),
        title="æ™ºæ…§çˆ¬è™«ç³»ç»Ÿ - è®©æ•°æ®æˆä¸ºAIçš„æ ¸å¿ƒåŠ¨åŠ›",
        css="""

        .gradio-container {
            width: 1400px;
            max-width: 1400px !important;
            margin: 0 auto !important;
            padding: 20px !important;
        }
        .metric-container {
            text-align: center;
        }
        .main-container {
            display: flex;
            justify-content: center;
            align-items: flex-start;
            min-height: 100vh;
            padding: 20px;
        }
        .content-wrapper {
            width: 100%;
            max-width: 1400px;
            margin: 0 auto;
        }
        """
    ) as demo:
        
        gr.Markdown("# ğŸ•·ï¸ æ™ºæ…§çˆ¬è™«ç³»ç»Ÿ - è®©æ•°æ®æˆä¸ºAIçš„æ ¸å¿ƒåŠ¨åŠ›")
        gr.Markdown("[Crawl4AI_PY](https://crawl4ai.renzhe.org) åŸºäº Crawl4AI çš„æ™ºèƒ½ç½‘é¡µçˆ¬è™«ï¼Œæ”¯æŒæ‰¹é‡æŠ“å–ã€æ•°æ®ç®¡ç†å’Œæ–‡ä»¶æµè§ˆ  [å¼€æºåœ°å€](https://github.com/zq535228/Crawl4AI_PY)")
        
        with gr.Tabs():
            # çˆ¬å–æ§åˆ¶æ ‡ç­¾é¡µ
            with gr.Tab("ğŸš€ çˆ¬å–æ§åˆ¶"):
                with gr.Row():
                    with gr.Column(scale=2):
                        gr.Markdown("### çˆ¬å–é…ç½®")
                        
                        url_input = gr.Textbox(
                            label="èµ·å§‹ URL",
                            placeholder="è¯·è¾“å…¥è¦çˆ¬å–çš„ç½‘é¡µ URLï¼Œä¾‹å¦‚ï¼šhttps://example.com",
                            value="https://www.msdmanuals.cn/professional/infectious-diseases"
                        )
                        
                        link_filter_input = gr.Textbox(
                            label="é“¾æ¥åŒ…å«å­—ç¬¦è¿‡æ»¤",
                            placeholder="è¾“å…¥å…³é”®è¯è¿‡æ»¤é“¾æ¥ï¼Œå¤šä¸ªå…³é”®è¯ç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼šnews,article,blog,professional/infectious-diseases",
                            value="/infectious-diseases"
                        )
                        
                        with gr.Row():
                            headless_checkbox = gr.Checkbox(
                                label="æ— å¤´æ¨¡å¼",
                                value=True,
                                info="å‹¾é€‰åæµè§ˆå™¨å°†åœ¨åå°è¿è¡Œï¼Œä¸æ˜¾ç¤ºç•Œé¢"
                            )
                            
                            max_depth_slider = gr.Slider(
                                minimum=1,
                                maximum=10,
                                value=3,
                                step=1,
                                label="æœ€å¤§é€’å½’æ·±åº¦",
                                info="æ§åˆ¶çˆ¬å–çš„é€’å½’æ·±åº¦ï¼Œé¿å…æ— é™é€’å½’"
                            )
                        
                        with gr.Row():
                            start_btn = gr.Button("ğŸš€ å¼€å§‹çˆ¬å–", variant="primary", size="lg")
                            stop_btn = gr.Button("â¹ï¸ åœæ­¢çˆ¬å–", variant="stop", size="lg")
                        
                        status_text = gr.Textbox(
                            label="çŠ¶æ€",
                            value="å°±ç»ª",
                            interactive=False
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### å®æ—¶ç»Ÿè®¡")
                        
                        with gr.Row():
                            total_metric = gr.Number(label="æ€»é“¾æ¥æ•°", value=0, precision=0)
                            success_metric = gr.Number(label="æˆåŠŸæ•°", value=0, precision=0)
                        
                        with gr.Row():
                            failed_metric = gr.Number(label="å¤±è´¥æ•°", value=0, precision=0)
                            pending_metric = gr.Number(label="å¾…å¤„ç†", value=0, precision=0)
                        
                        success_rate_metric = gr.Number(label="æˆåŠŸç‡(%)", value=0, precision=1)
                        
                        refresh_stats_btn = gr.Button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡", size="sm")
                        clear_logs_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ—¥å¿—", variant="secondary", size="sm")
                
                gr.Markdown("### å®æ—¶æ—¥å¿—")
                logs_output = gr.Textbox(
                    label="çˆ¬å–æ—¥å¿—",
                    lines=15,
                    max_lines=30,
                    interactive=False,
                    show_copy_button=True,
                    autoscroll=True  # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
                )
                
                # äº‹ä»¶ç»‘å®š
                start_btn.click(
                    fn=start_crawling_and_update_buttons,
                    inputs=[url_input, headless_checkbox, link_filter_input, max_depth_slider],
                    outputs=[status_text, logs_output, start_btn, stop_btn]
                )
                
                stop_btn.click(
                    fn=stop_crawling_and_update_buttons,
                    outputs=[status_text, start_btn, stop_btn]
                )
                
                refresh_stats_btn.click(
                    fn=get_statistics,
                    outputs=[total_metric, success_metric, failed_metric, pending_metric, success_rate_metric]
                )
                
                clear_logs_btn.click(
                    fn=clear_crawling_logs,
                    outputs=[logs_output]
                )
                
                # å®æ—¶æ—¥å¿—æ›´æ–°åŠŸèƒ½
                def update_logs_stats_and_buttons():
                    """æ›´æ–°æ—¥å¿—ã€ç»Ÿè®¡ä¸æŒ‰é’®å¯äº¤äº’çŠ¶æ€"""
                    logs = get_crawling_logs()
                    stats = get_statistics()
                    start_update, stop_update = get_buttons_state()
                    return (
                        logs,
                        stats[0], stats[1], stats[2], stats[3], stats[4],
                        start_update, stop_update
                    )
                
                # ä½¿ç”¨å®šæ—¶å™¨ç»„ä»¶å®ç°å®æ—¶æ›´æ–°
                timer = gr.Timer(value=2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
                timer.tick(
                    fn=update_logs_stats_and_buttons,
                    outputs=[
                        logs_output, total_metric, success_metric, failed_metric, pending_metric, success_rate_metric,
                        start_btn, stop_btn
                    ]
                )
                # é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–æŒ‰é’®äº¤äº’çŠ¶æ€
                demo.load(
                    fn=get_buttons_state,
                    outputs=[start_btn, stop_btn]
                )

            # æ•°æ®ç»Ÿè®¡æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ“Š æ•°æ®ç»Ÿè®¡"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ç»Ÿè®¡æ¦‚è§ˆ")
                        
                        stats_total = gr.Number(label="æ€»é“¾æ¥æ•°", value=0, precision=0)
                        stats_success = gr.Number(label="æˆåŠŸæ•°", value=0, precision=0)
                        stats_failed = gr.Number(label="å¤±è´¥æ•°", value=0, precision=0)
                        stats_pending = gr.Number(label="å¾…å¤„ç†", value=0, precision=0)
                        stats_rate = gr.Number(label="æˆåŠŸç‡(%)", value=0, precision=1)
                        
                        refresh_stats_btn2 = gr.Button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡", variant="secondary")
                    
                    with gr.Column(scale=2):
                        gr.Markdown("### çŠ¶æ€åˆ†å¸ƒå›¾")
                        plot_output = gr.Plot(label="æŠ“å–çŠ¶æ€åˆ†å¸ƒ")
                
                gr.Markdown("### æœ€è¿‘æŠ“å–çš„é“¾æ¥")
                recent_links_df = gr.Dataframe(
                    label="æœ€è¿‘é“¾æ¥",
                    headers=['URL', 'æ ‡é¢˜', 'çŠ¶æ€', 'å‘ç°æ—¶é—´', 'æŠ“å–æ—¶é—´', 'é”™è¯¯ä¿¡æ¯'],
                    interactive=True,
                    wrap=False,
                    column_widths=[900, 250, 100, 180, 180, 300],
                    datatype=["str", "str", "str", "str", "str", "str"],
                    max_height=1000
                )
                
                # äº‹ä»¶ç»‘å®š
                refresh_stats_btn2.click(
                    fn=get_statistics,
                    outputs=[stats_total, stats_success, stats_failed, stats_pending, stats_rate]
                )
                
                refresh_stats_btn2.click(
                    fn=create_statistics_plot,
                    outputs=[plot_output]
                )
                
                refresh_stats_btn2.click(
                    fn=lambda: get_links_by_status('success'),
                    outputs=[recent_links_df]
                )
                
                # é¡µé¢åŠ è½½æ—¶è‡ªåŠ¨åˆ·æ–°
                demo.load(
                    fn=get_statistics,
                    outputs=[stats_total, stats_success, stats_failed, stats_pending, stats_rate]
                )
                
                demo.load(
                    fn=create_statistics_plot,
                    outputs=[plot_output]
                )
                
                demo.load(
                    fn=lambda: get_links_by_status('success'),
                    outputs=[recent_links_df]
                )
            
            # é“¾æ¥ç®¡ç†æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ”— é“¾æ¥ç®¡ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ç­›é€‰å’Œæœç´¢")
                        
                        status_filter = gr.Dropdown(
                            choices=['None', 'pending', 'success', 'failed'],
                            value='None',
                            label="æŒ‰çŠ¶æ€ç­›é€‰",
                            info="é€‰æ‹©è¦æŸ¥çœ‹çš„é“¾æ¥çŠ¶æ€"
                        )
                        
                        search_input = gr.Textbox(
                            label="æœç´¢ URL",
                            placeholder="è¾“å…¥å…³é”®è¯æœç´¢é“¾æ¥...",
                            info="æ”¯æŒ URL æ¨¡ç³Šæœç´¢"
                        )
                        
                        with gr.Row():
                            search_btn = gr.Button("ğŸ” æœç´¢", variant="primary")
                            retry_btn = gr.Button("ğŸ”„ é‡è¯•å¤±è´¥é“¾æ¥", variant="secondary")
                        
                        with gr.Row():
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰é“¾æ¥", variant="stop", size="sm")
                            confirm_clear_btn = gr.Button("âœ… ç¡®è®¤æ¸…ç©º", variant="stop", size="sm", visible=False)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### æ“ä½œç»“æœ")
                        operation_result = gr.Textbox(
                            label="æ“ä½œç»“æœ",
                            interactive=False,
                            lines=3
                        )
                
                gr.Markdown("### é“¾æ¥åˆ—è¡¨")
                links_df = gr.Dataframe(
                    label="é“¾æ¥åˆ—è¡¨",
                    headers=['URL', 'æ ‡é¢˜', 'çŠ¶æ€', 'å‘ç°æ—¶é—´', 'æŠ“å–æ—¶é—´', 'é”™è¯¯ä¿¡æ¯'],
                    interactive=True,
                    wrap=False,
                    column_widths=[900, 250, 100, 180, 180, 300],
                    datatype=["str", "str", "str", "str", "str", "str"],
                    max_height=1000
                )
                
                # äº‹ä»¶ç»‘å®š
                status_filter.change(
                    fn=get_links_by_status,
                    inputs=[status_filter],
                    outputs=[links_df]
                )
                
                search_btn.click(
                    fn=search_links,
                    inputs=[search_input],
                    outputs=[links_df]
                )
                
                retry_btn.click(
                    fn=retry_failed_links,
                    outputs=[operation_result]
                )
                
                def show_confirm_clear():
                    """æ˜¾ç¤ºç¡®è®¤æ¸…ç©ºæŒ‰é’®"""
                    return gr.update(visible=True)
                
                def hide_confirm_clear():
                    """éšè—ç¡®è®¤æ¸…ç©ºæŒ‰é’®"""
                    return gr.update(visible=False)
                
                clear_btn.click(
                    fn=clear_all_links,
                    outputs=[operation_result]
                ).then(
                    fn=show_confirm_clear,
                    outputs=[confirm_clear_btn]
                )
                
                confirm_clear_btn.click(
                    fn=confirm_clear_links,
                    outputs=[operation_result]
                ).then(
                    fn=hide_confirm_clear,
                    outputs=[confirm_clear_btn]
                )
                
                # é¡µé¢åŠ è½½æ—¶æ˜¾ç¤ºæ‰€æœ‰é“¾æ¥
                demo.load(
                    fn=lambda: get_links_by_status(),
                    outputs=[links_df]
                )
            
            # æ–‡ä»¶æµè§ˆæ ‡ç­¾é¡µ
            with gr.Tab("ğŸ“ æ–‡ä»¶æµè§ˆ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æ–‡ä»¶åˆ—è¡¨")
                        
                        file_list = gr.Dropdown(
                            choices=get_output_files(),
                            label="é€‰æ‹©æ–‡ä»¶",
                            info="é€‰æ‹©è¦é¢„è§ˆçš„æ–‡ä»¶"
                        )
                        
                        with gr.Row():
                            refresh_files_btn = gr.Button("ğŸ”„ åˆ·æ–°æ–‡ä»¶åˆ—è¡¨", size="sm")
                            download_file_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ–‡ä»¶", size="sm", variant="secondary")
                            clear_files_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶", variant="stop", size="sm")
                            confirm_clear_files_btn = gr.Button("âœ… ç¡®è®¤æ¸…ç©º", variant="stop", size="sm", visible=False)
                    
                    with gr.Column(scale=2):
                        gr.Markdown("### æ–‡ä»¶é¢„è§ˆ")
                        
                        # æ·»åŠ é¢„è§ˆæ¨¡å¼é€‰æ‹©
                        preview_mode = gr.Radio(
                            choices=["Markdown æ¸²æŸ“", "åŸå§‹æ–‡æœ¬"],
                            value="Markdown æ¸²æŸ“",
                            label="é¢„è§ˆæ¨¡å¼",
                            info="é€‰æ‹©æ–‡ä»¶å†…å®¹çš„æ˜¾ç¤ºæ–¹å¼"
                        )
                        
                        file_preview = gr.Markdown(
                            label="æ–‡ä»¶å†…å®¹",
                            value="è¯·é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œé¢„è§ˆ",
                            show_copy_button=True
                        )
                        
                        gr.Markdown("### æ“ä½œç»“æœ")
                        file_operation_result = gr.Textbox(
                            label="æ“ä½œç»“æœ",
                            interactive=False,
                            lines=3
                        )
                
                # äº‹ä»¶ç»‘å®š
                def update_preview(file_path, mode):
                    """æ›´æ–°æ–‡ä»¶é¢„è§ˆ"""
                    return preview_file(file_path, mode)
                
                file_list.change(
                    fn=update_preview,
                    inputs=[file_list, preview_mode],
                    outputs=[file_preview]
                )
                
                preview_mode.change(
                    fn=update_preview,
                    inputs=[file_list, preview_mode],
                    outputs=[file_preview]
                )
                
                def refresh_file_list():
                    """åˆ·æ–°æ–‡ä»¶åˆ—è¡¨"""
                    files = get_output_files()
                    return gr.update(choices=files, value=None)
                
                def show_confirm_clear_files():
                    """æ˜¾ç¤ºç¡®è®¤æ¸…ç©ºæ–‡ä»¶æŒ‰é’®"""
                    return gr.update(visible=True)
                
                def hide_confirm_clear_files():
                    """éšè—ç¡®è®¤æ¸…ç©ºæ–‡ä»¶æŒ‰é’®"""
                    return gr.update(visible=False)
                
                refresh_files_btn.click(
                    fn=refresh_file_list,
                    outputs=[file_list]
                )
                
                download_file_btn.click(
                    fn=download_file,
                    inputs=[file_list],
                    outputs=gr.File(label="ä¸‹è½½æ–‡ä»¶")
                )
                
                clear_files_btn.click(
                    fn=clear_all_files,
                    outputs=[file_operation_result]
                ).then(
                    fn=show_confirm_clear_files,
                    outputs=[confirm_clear_files_btn]
                )
                
                confirm_clear_files_btn.click(
                    fn=confirm_clear_files,
                    outputs=[file_operation_result]
                ).then(
                    fn=hide_confirm_clear_files,
                    outputs=[confirm_clear_files_btn]
                ).then(
                    fn=refresh_file_list,
                    outputs=[file_list]
                )
                
                # é¡µé¢åŠ è½½æ—¶è·å–æ–‡ä»¶åˆ—è¡¨
                demo.load(
                    fn=refresh_file_list,
                    outputs=[file_list]
                )
    
    return demo




def main():
    """ä¸»å‡½æ•°"""
    print("æ­£åœ¨å¯åŠ¨çˆ¬è™«ç®¡ç†ç³»ç»Ÿ...")
    
    # åˆ›å»ºç•Œé¢
    demo = create_interface()

    # ç«¯å£é…ç½®é€»è¾‘
    # æ£€æµ‹æ˜¯å¦åœ¨ Docker ç¯å¢ƒä¸­è¿è¡Œï¼ˆä½¿ç”¨å¯¼å…¥çš„å‡½æ•°ï¼‰
    
    # æ ¹æ®è¿è¡Œç¯å¢ƒé€‰æ‹©ç«¯å£
    if is_docker_environment():
        server_port = 7861
        print("ğŸ³ æ£€æµ‹åˆ° Docker ç¯å¢ƒï¼Œä½¿ç”¨ç«¯å£: 7861")
    else:
        server_port = 7862
        print("ğŸ’» æ£€æµ‹åˆ°æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ç«¯å£: 7862")
    
    # å¯åŠ¨åº”ç”¨
    try:
        demo.launch(
            server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
            server_port=server_port,       # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
            share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
            debug=True,             # å¼€å¯è°ƒè¯•æ¨¡å¼
            show_error=True,        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            quiet=False             # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ åº”ç”¨ç¨‹åºå·²åœæ­¢")


if __name__ == "__main__":
    main()
